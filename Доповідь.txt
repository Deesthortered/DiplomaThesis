1. Титулка (0:10)
Доброго дня, мене звати Давидько Анатолій, тема дипломної роботи "ЗАСТОСУВАННЯ МЕТОДІВ НАВЧАННЯ З ПІДКРІПЛЕННЯМ ДЛЯ НАВЧАННЯ АГЕНТІВ ІГРОВИХ СЕРЕДОВИЩ"

2. Задача (0:45)
Метою дипломної роботи є дослідження ефективності алгоритмів глибокого навчання з підкріпленням для пошуку оптимального керування агентів в дифференційній грі переслідування.
Для досягення мети було визначення наступні завдання:
	1. Проаналізувати диференційну гру, визначити її основні вимоги та ознаки оптимальної стратегії.
	2. Створити гнучку програмну реалізацію середовища для дослідження алгоритмів пошуку оптимальної стратегії.
	3. Проаналізувати та реалізувати алгоритми глибокого навчання з підкріпленням.
	4. Дослідити ефективність алгоритмів на програмній реалізації середовища в різних її умовах.


3. Актуальність (1:23)
Вперше свою актуальність диференційні ігри продемонстрували ще в 60-х роках минолого сторіччя, коли радянські військові вивчали питання 
теорії оптимального керування у військових цілях. Наприклад, задача перехвату літака ракетою та уникнення літаком ракети, чи переслідування торпедою
катера та уникнення катером попадання.
З тих пір диференційні ігри розширили своє застосування в іншіх галузях діяльності людини, а саме в економічній сфері.
Наприклад, теорію застосовують для аналізу та передбачення поведінки олігополій.
Також з часом зв'явилися методи навчання з підкріпленням, які розвязують подібні задачі з одним агентом і були сильно розвинуті за минуле десятиріччя.
Тому виникає інтерес до використання цих методів для диференційних ігор.


4. Диференційні ігри (2:00)
Розглянемо диференційну гру.
В одній системі координат знаходяться два керовані об'єкти, які умовно назвемо хижаком та жертвою.
Кожен з них володіє положенням, швидкістю (яка обмежена зверху) та статичним прискоренням.
Ці величини складають фазові координати керованих об'єктів.
Метою хижака є за обмежений час набути такі ж координати положення як жертва (тобто піймати її).
У жертви мета не допустити цього в продовж всього наданого часу.
Якісною їх різницею є значення максимальної швидкості, яка менша у жертви, і прискорення, яке менше у хижака, що робить пошук стратегії для обох нетривіальною задачею.
Метою гри є пошук оптимальних керувань для об'єктів.


5. Елементи навчання з підкріпленням (2:30)
Ця система є еквівалентною системі елементів навчання з підкріплення, у якій
існує агент, яким ми керуємо, і середовище, в якому відбуваються події гри.
Агент також має стан, який виражається положенням, швидкістю і прискоренням, і дії, 
які виражаються векторами напрямку. Але в цій системі немає явно вираженої мети.
Замість цього існує поняття нагороди, яка надається кожен момент часу і чисельно оцінює виконану дію агентом в певному стані.
Задачею навчання з підкріпленням є максимізація сумарної нагороди за весь відведений час.


6. Методи навчання з підкріпленням
Методи навчання з підкріпленням почали свій розвиток з динамічного програмування і опираються на принцип оптимальності Белмана та на його рівняння.
Він полягає в тому, що на кожному кроці необхідно формувати оптимальну стратегію в припущенні про оптимальність всіх наступних кроків.
Вони визначають поняття функцій корисності, які для більшості алгоритмів є обов'язковою. Ці функції чисельно виражають наскільки 
вигідною є певний стан чи дія виконана в певному стані. Серед класичних алгоритмів навчання з підкріпленням виділяють 
метод Q-learning, який формує стратегію на жадібному виборі дії за значенням функції корисності. Ця функція визначена як
середнє значення нагороди цієї дії з урахуванням оцінки наступної дії згідно принципу оптимальності Белмана.


7. Глибоке навчання з підкріпленням (Deep RL)
Вдосконалення класичних методів виражається в реалізації більш гнучких математичних моделей для формування функцій корисності.
В якості таких моделей використовують нейронні мережі, які дозволяють не просто формувати оцінки для конкретних прикладів станів та дій,
а знаходити та узагальнювати шукану залежність між ними. Серед таких методів буде розглянутий Deep Q-learning, який є очевидним вдосконаленям 
базового Q-learning та метод Advantage Actor-Critic. Ідея останього полягає в використання середньої оцінки стану незалежно від дії та 
оцінювання дії відносно попередньої оцінки. Це надає суттєву перевагу при виборі оптимальної дії, коли всі інші мають оцінки тільки позитивні або негативні,
і є необхідність відносного оцінювання та вибору "найменшого зла".


8. Програмна реалізація середовища
Для дослідження ефективності цих методів була розроблена спеціальна програма, яка реалізує в собі середовище диференційної гри.
Вона є кросплатфоменою і надає зручний та гнучкий інтрейфейс, який дозволяє керувати процесом навчання. Вона має гнучку архітектуру, яка
дозволяє додавати нові алгоритми до програми без необхідності модифікації коду процесу навчання. Це досягається завдяки узагальненю 
взаємодії компонент до визначеного інтерфейсу. До того ж є можливість вибирати та змінювати алгоритми прямо під час роботи програми,
яка називається хотсвапом. 


9. Застосовані конфігурації
Ця програма була створена таким чином, щоб можна було застосовувати різні конфгурації представлення станів та нагород.
Дії виражаються множиною з восьми векторів напрямку та нульового вектору яким визначається бездіяльність.
Стани можуть виражені координатами положення агентів та векторами їх швидкості (позначимо S1) 
!або! векторами напрямку від хижака до жертви та векторами їх швидкості (позначимо S2).
Нагороди для агентів можно обрати статичні - які надають постійне негативне значення хижаку і позитивне жертві (позначимо R-)
!або! динамічні, які залежать від зміни дистанції між ними (позначимо R+).


10. Метрики якості
Ефективність алгоритмів буде визначатися певними вимірюваними величинами, які умовно були названі метриками.
В якості метрик було використано наступні величини:
- Відношення кількості епізодів, які закінчилися зустріччю агентів, до загальної кількості пройдених епізодів
- Тривалість епізоду
- Сумарна нагорода хижака за епізод
- Сумарна нагорода жертви за епізод
Вони дозволяють показати успішність роботи алгоритмів з точки зору поставленої задачі користувачем та її розуміння середовищем.


11. Експеримент 1
Мета першого експеримента є підтвердити можливість алгоритмів взагалі вирішувати задачу зі сторони хижака.
Тому в цьому випадку вибрана конфігурація S2 і R+, жертва є нерухомою, що значно спрощує поставлену задачу. 
Візуальне зображення експерименту можна спостерігати на анімації праворуч.
A2C досить швидко знайшов ефективну стратегію і наблизився до 100% успішних епізодів, що означає здатність алгоритму вирішувати цю задачу.
Тривалість епізоду з часом зменшувалась і це означає, що хижак все швидше досягав мети і його стратегія з часом позбавлялася неефективних проміжкових дій.
Алгоритм DQN не знайшов ефективної стратегії для вирішення задачі, що зрозуміло по відсотку успішних ігор.
Графік сумарної нагороди не є рівномірним, що свідчить про нестабільність процесу пошуку оптимальної стратегії алгоритмом.


12. Експеримент 2
Мета другого експерименту є підтвердити можливість алгоритмів взагалі вирішувати задачу зі сторони жертви.
Тому в цьому випадку вибрана конфігурація S2 і R+, хижак керується алгоритмом А2С, а максимальні швидкості агентів зрівняні, що значно спрощує поставлену задачу.
Алгоритм DQN не зміг знайти стратегію для досягнення поставленої мети. 
Частота зустрічей хижака і жертви швидко збігається до 100% і тривалість епізоду достатньо низька, що приводить до наведеного висновку.
Алгоритм A2C зміг знайти достатньо ефективну стратегію, але її не можна назвати оптимальною. 
Частота зустрічі хижака і жертви впродовж експерименту залишалася на рівні 30%, що є непоганим результатом. 
Враховуючи, що жертва мала тільки вектори напрямку в якості наданих станів, вона не може оцінити дистанцію між агентами, тому це є достатньо ефективною стратегією


13. Експеримент 3, 4, 5, 6
Мета цих еспериментів є порівняння ефективності алгоритмів для пошуку оптимального керування в різних конфігураціях, які наведені на слайді.
В кожному випадку був обраний алгоритм А2С для обох агентів, який на відміну від DQN здатний вирішувати задачу. 
Третій та четвертий експеримент показують, що використання конфігурації R- не дає можливості агентам адектватно зрозуміти поставлену задачу, на відміну від R+.
Про це свідчать низький показник відсотку кількості зустрічів і постійно високі по модулю значення нагород для агентів, 
які псують процес навчання та унеможливлють пошук послідовності ефективних дій з фінальною шуканою нагородою.
За даними п'ятого та шостого експериментів можна спостерігати успішний пошук стратегії хижаком в продовж всього експерименту, 
але при цьому і жертва продемонструвала задовільний результат, що видно графіку кількості зустрічів.
Дані про тривалість епізоду є досить нерівномірними, що свідчить про активний і успішний пошук стратегій агентами і чергуванні переваги одного агента над іншим.
Але конфігурація S2 позбавила жертву можливості оцінювати дистанцію між агентами в обмін на легше сприйняття відносного положення. 
Таким чином процес пошуку оптимальної стратегії в шостому експерименті жертвою погіршився. Хижак використовує цю ваду і отримує кращі результати. 


14. Експеримент 7, 8
Незважаючи на погані результати алгоритму DQN в експерименті перевірки здатності вирішувати задачу, ці результати не були нульовими. 
До того ж використовуючи інший підхід до пошуку стратегії процес рішення задачі агентами може значно відрізнятися. 
Тому виникає необхідність виконати подібні експерименти, щоб виявити нові залежності або підтвердити попередні.
Мета цих еспериментів є більш детальне дослілження ефективності алгоритму DQN в ролі жертви при найсприятливіших умовах.
Тому в цьому випадку вибрана конфігурація R+ разом з S1 і S2 попарно, хижак керується алгоритмом А2С.
Показник відсотку успішних для хижака епізодів показує різницю між результатами цих експериментів.
При використані конфігурації S1 можна спостерігати покращення результатів жертви, що знову 
підтверджує перевагу такої конфігурації для пошуку оптимальної стратегії жертви.


15. Висновки
Що ми маємо в підсумку? ---(дальше тупо по слайду)
Була розроблена програмна реалізація середовища
Були реалізовані алгоритми машинного навчання з підкріпленням
Була проаналізована ефективність даних методів
Було отримано результати, які приводять до наступних висновків

A2C ефективно вирішує задачу хижака і жертви
DQN не є ефективним алгоритмом для хижака
Надання динамічної нагороди краще описує поставлену задачу
Стани представлені у вигляді координат краще виражають систему для пошуку оптимальної стратегії жертвою
Стани представлені у вигляді вектору напрямку спрощують задачу пошуку ефективної стратегії хижаком, але не надають необхідної інформації для оптимальної стратегії

16. Дякую за увагу
Дякую за увагу



